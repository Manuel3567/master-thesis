{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "InferenceConfig(adaptive_max_seq_len_to_max_full_table_size=300000, \n",
    "batch_size=2, \n",
    "emsize=192, \n",
    "features_per_group=2, max_num_classes=0, nhead=6, remove_duplicate_features=False, seq_len=4000, \n",
    "task_type='regression', num_buckets=5000, max_num_features=85, two_sets_of_queries=None, \n",
    "aggregate_k_gradients=1, differentiable_hps_as_style=False, dropout=0.0, encoder_use_bias=False, feature_positional_embedding='subspace', \n",
    "multiquery_item_attention=False, nan_handling_enabled=True, nan_handling_y_encoder=True, nhid_factor=4, nlayers=12, \n",
    "normalize_by_used_features=True, normalize_on_train_only=True, normalize_to_ranking=False, normalize_x=True, num_global_att_tokens=0, \n",
    "progress_bar=False, recompute_attn=False, recompute_layer=True, remove_empty_features=True, remove_outliers=False, \n",
    "semisupervised_enabled=False, timing=False, use_separate_decoder=False, use_flash_attention=True, multi_query_factor=None, \n",
    "multiquery_item_attention_for_test_set=True, attention_init_gain=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerFeatureTransformer(\n",
       "  (encoder): SequentialEncoder(\n",
       "    (0): RemoveEmptyFeaturesEncoderStep()\n",
       "    (1): NanHandlingEncoderStep()\n",
       "    (2): VariableNumFeaturesEncoderStep()\n",
       "    (3): InputNormalizationEncoderStep()\n",
       "    (4): VariableNumFeaturesEncoderStep()\n",
       "    (5): LinearInputEncoderStep(\n",
       "      (layer): Linear(in_features=4, out_features=192, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (y_encoder): SequentialEncoder(\n",
       "    (0): NanHandlingEncoderStep()\n",
       "    (1): LinearInputEncoderStep(\n",
       "      (layer): Linear(in_features=2, out_features=192, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (transformer_encoder): LayerStack(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x PerFeatureEncoderLayer(\n",
       "        (self_attn_between_features): MultiHeadAttention()\n",
       "        (self_attn_between_items): MultiHeadAttention()\n",
       "        (mlp): MLP(\n",
       "          (linear1): Linear(in_features=192, out_features=768, bias=False)\n",
       "          (linear2): Linear(in_features=768, out_features=192, bias=False)\n",
       "        )\n",
       "        (layer_norms): ModuleList(\n",
       "          (0-2): 3 x LayerNorm((192,), eps=1e-05, elementwise_affine=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_dict): ModuleDict(\n",
       "    (standard): Sequential(\n",
       "      (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=768, out_features=5000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_positional_embedding_embeddings): Linear(in_features=48, out_features=192, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "PerFeatureTransformer                              --\n",
       "├─SequentialEncoder: 1-1                           --\n",
       "│    └─RemoveEmptyFeaturesEncoderStep: 2-1         --\n",
       "│    └─NanHandlingEncoderStep: 2-2                 --\n",
       "│    └─VariableNumFeaturesEncoderStep: 2-3         --\n",
       "│    └─InputNormalizationEncoderStep: 2-4          --\n",
       "│    └─VariableNumFeaturesEncoderStep: 2-5         --\n",
       "│    └─LinearInputEncoderStep: 2-6                 --\n",
       "│    │    └─Linear: 3-1                            768\n",
       "├─SequentialEncoder: 1-2                           --\n",
       "│    └─NanHandlingEncoderStep: 2-7                 --\n",
       "│    └─LinearInputEncoderStep: 2-8                 --\n",
       "│    │    └─Linear: 3-2                            576\n",
       "├─LayerStack: 1-3                                  --\n",
       "│    └─ModuleList: 2-9                             --\n",
       "│    │    └─PerFeatureEncoderLayer: 3-3            589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-4            589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-5            589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-6            589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-7            589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-8            589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-9            589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-10           589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-11           589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-12           589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-13           589,824\n",
       "│    │    └─PerFeatureEncoderLayer: 3-14           589,824\n",
       "├─ModuleDict: 1-4                                  --\n",
       "│    └─Sequential: 2-10                            --\n",
       "│    │    └─Linear: 3-15                           148,224\n",
       "│    │    └─GELU: 3-16                             --\n",
       "│    │    └─Linear: 3-17                           3,845,000\n",
       "├─Linear: 1-5                                      9,408\n",
       "===========================================================================\n",
       "Total params: 11,081,864\n",
       "Trainable params: 11,081,864\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model.model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.83  , 5.19  ],\n",
       "       [2.825 , 5.16  ],\n",
       "       [2.82  , 5.13  ],\n",
       "       [2.815 , 5.1   ],\n",
       "       [2.81  , 5.07  ],\n",
       "       [2.835 , 5.0525],\n",
       "       [2.86  , 5.035 ],\n",
       "       [2.885 , 5.0175],\n",
       "       [2.91  , 5.    ],\n",
       "       [2.9975, 5.025 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(n).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 5000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "cat_ix = []\n",
    "style = None\n",
    "X_full = X_train\n",
    "X_full = torch.cat([torch.tensor(X_train.head(n).values, dtype=torch.float32), torch.tensor(X_validation.head(n).values, dtype=torch.float32)], dim=0).unsqueeze(1)\n",
    "\n",
    "model.model_(\n",
    "                    *(style, X_full, torch.tensor(y_train.head(n).values, dtype=torch.float32)),\n",
    "                    only_return_standard_out=True,\n",
    "                    categorical_inds=cat_ix,\n",
    "                    single_eval_pos=n\n",
    "                ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8300, 5.1900],\n",
       "        [2.8250, 5.1600],\n",
       "        [2.8200, 5.1300],\n",
       "        [2.8150, 5.1000],\n",
       "        [2.8100, 5.0700],\n",
       "        [2.8350, 5.0525],\n",
       "        [2.8600, 5.0350],\n",
       "        [2.8850, 5.0175],\n",
       "        [2.9100, 5.0000],\n",
       "        [2.9975, 5.0250]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X_train.head(n).values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, int, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(X_train\u001b[38;5;241m.\u001b[39mhead(n)\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: tensors used as indices must be long, int, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "for k in torch.tensor(X_train.head(n).values, dtype=torch.float32):\n",
    "    print(torch.tensor(X_train.head(n).values, dtype=torch.float32)[k])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = PerFeatureTransformer(\n",
    "        seed=model_seed,\n",
    "        # Things that were explicitly passed inside `build_model()`\n",
    "        encoder=get_encoder(\n",
    "            num_features=config.features_per_group,\n",
    "            embedding_size=config.emsize,\n",
    "            remove_empty_features=config.remove_empty_features,\n",
    "            remove_duplicate_features=config.remove_duplicate_features,\n",
    "            nan_handling_enabled=config.nan_handling_enabled,\n",
    "            normalize_on_train_only=config.normalize_on_train_only,\n",
    "            normalize_to_ranking=config.normalize_to_ranking,\n",
    "            normalize_x=config.normalize_x,\n",
    "            remove_outliers=config.remove_outliers,\n",
    "            normalize_by_used_features=config.normalize_by_used_features,\n",
    "            encoder_use_bias=config.encoder_use_bias,\n",
    "        ),\n",
    "        y_encoder=get_y_encoder(\n",
    "            num_inputs=1,\n",
    "            embedding_size=config.emsize,\n",
    "            nan_handling_y_encoder=config.nan_handling_y_encoder,\n",
    "            max_num_classes=config.max_num_classes,\n",
    "        ),\n",
    "        nhead=config.nhead,\n",
    "        ninp=config.emsize,\n",
    "        nhid=config.emsize * config.nhid_factor,\n",
    "        nlayers=config.nlayers,\n",
    "        features_per_group=config.features_per_group,\n",
    "        cache_trainset_representation=True,\n",
    "        #\n",
    "        # Based on not being present in config or otherwise, these were default values\n",
    "        init_method=None,\n",
    "        decoder_dict={\"standard\": (None, n_out)},\n",
    "        use_encoder_compression_layer=False,\n",
    "        #\n",
    "        # These were extra things passed in through `**model_extra_args`\n",
    "        # or `**extra_model_kwargs` and were present in the config\n",
    "        recompute_attn=config.recompute_attn,\n",
    "        recompute_layer=config.recompute_layer,\n",
    "        feature_positional_embedding=config.feature_positional_embedding,\n",
    "        use_separate_decoder=config.use_separate_decoder,\n",
    "        #\n",
    "        # These are things that had default values from config.get() but were not\n",
    "        # present in any config.\n",
    "        layer_norm_with_elementwise_affine=False,\n",
    "        nlayers_decoder=None,\n",
    "        pre_norm=False,\n",
    "        #\n",
    "        # These seem to map to `**layer_config` in the init of `PerFeatureTransformer`\n",
    "        # Which got passed to the `PerFeatureEncoderLayer(**layer_config)`\n",
    "        multiquery_item_attention=config.multiquery_item_attention,  # False\n",
    "        multiquery_item_attention_for_test_set=config.multiquery_item_attention_for_test_set,  # True  # noqa: E501\n",
    "        # Is either 1.0 or None in the configs, which lead to the default of 1.0 anywho\n",
    "        attention_init_gain=(\n",
    "            config.attention_init_gain\n",
    "            if config.attention_init_gain is not None\n",
    "            else 1.0\n",
    "        ),\n",
    "        # Is True, False in the config or not present,\n",
    "        # with the default of the `PerFeatureEncoderLayer` being False,\n",
    "        # which is what the value would have mapped to if the config had not present\n",
    "        two_sets_of_queries=(\n",
    "            config.two_sets_of_queries\n",
    "            if config.two_sets_of_queries is not None\n",
    "            else False\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline(steps=[('input_transformer', SafePowerTransformer()),\n",
    "                ('standard',\n",
    "                 Pipeline(steps=[('inf_to_nan_pre ',\n",
    "                                  FunctionTransformer(check_inverse=False,\n",
    "                                                      func=<function _inf_to_nan_func at 0x000001CA0EB959E0>,\n",
    "                                                      inverse_func=<function _identity at 0x000001CA0EB95BC0>)),\n",
    "                                 ('nan_impute_pre ',\n",
    "                                  SimpleImputer(keep_empty_features=True)),\n",
    "                                 ('standard', StandardScaler()),\n",
    "                                 ('inf_to_nan_post',\n",
    "                                  FunctionTransformer(check_inverse=False,\n",
    "                                                      func=<function _inf_to_nan_func at 0x000001CA0EB959E0>,\n",
    "                                                      inverse_func=<function _identity at 0x000001CA0EB95BC0>)),\n",
    "                                 ('nan_impute_post',\n",
    "                                  SimpleImputer(keep_empty_features=True))]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ColumnTransformer(sparse_threshold=0.0,\n",
    "                  transformers=[('original', 'passthrough', [0, 1]),\n",
    "                                ('feat_transform',\n",
    "                                 QuantileTransformer(n_quantiles=2,\n",
    "                                                     random_state=1894655253),\n",
    "                                 [0, 1])])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "FeatureUnion(transformer_list=[('passthrough',\n",
    "                                FunctionTransformer(func=<function _identity at 0x0000028758975C60>)),\n",
    "                               ('svd',\n",
    "                                Pipeline(steps=[('save_standard',\n",
    "                                                 Pipeline(steps=[('inf_to_nan_pre ',\n",
    "                                                                  FunctionTransformer(check_inverse=False,\n",
    "                                                                                      func=<function _inf_to_nan_func at 0x0000028758975A80>,\n",
    "                                                                                      inverse_func=<function _identity at 0x0000028758975C60>)),\n",
    "                                                                 ('nan_impute_pre...\n",
    "                                                                 ('standard',\n",
    "                                                                  StandardScaler(with_mean=False)),\n",
    "                                                                 ('inf_to_nan_post',\n",
    "                                                                  FunctionTransformer(check_inverse=False,\n",
    "                                                                                      func=<function _inf_to_nan_func at 0x0000028758975A80>,\n",
    "                                                                                      inverse_func=<function _identity at 0x0000028758975C60>)),\n",
    "                                                                 ('nan_impute_post',\n",
    "                                                                  SimpleImputer(keep_empty_features=True))])),\n",
    "                                                ('svd',\n",
    "                                                 TruncatedSVD(algorithm='arpack',\n",
    "                                                              n_components=1,\n",
    "                                                              random_state=1894655253))]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_components=max(1, min(num_examples // 10 + 1, num_features // 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
