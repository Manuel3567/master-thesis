{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f19f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "\n",
    "from analysis.datasets import load_entsoe\n",
    "from analysis.splits import to_train_validation_test_data\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, target_column=\"power\"):\n",
    "        self.target_column = target_column\n",
    "        self.max_power_value_rounded = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads the dataset.\"\"\"\n",
    "        self.df = load_entsoe()\n",
    "        return self\n",
    "\n",
    "    def transform_power(self, epsilon=1e-3):\n",
    "        \"\"\"Scales the power data using log transformation.\"\"\"\n",
    "        max_power_value = self.df[self.target_column].max()\n",
    "        self.max_power_value_rounded = np.ceil(max_power_value / 1000) * 1000\n",
    "        self.df[self.target_column] = np.log(self.df[self.target_column] / self.max_power_value_rounded + epsilon)\n",
    "        return self\n",
    "\n",
    "    def add_interval_index(self):\n",
    "        \"\"\"Creates an interval index feature based on time.\"\"\"\n",
    "        self.df['interval_index'] = ((self.df.index.hour * 60 + self.df.index.minute) // 15) + 1\n",
    "        return self\n",
    "\n",
    "    def add_lagged_features(self, lag=96):\n",
    "        \"\"\"Adds lagged power feature.\"\"\"\n",
    "        self.df[f'{self.target_column}_t-{lag}'] = self.df[self.target_column].shift(lag)\n",
    "        self.df.dropna(inplace=True)\n",
    "        return self\n",
    "\n",
    "    def prepare_features(self, selected_features):\n",
    "        \"\"\"Selects only the specified features from the DataFrame.\"\"\"\n",
    "        selected_features.append(self.target_column)\n",
    "        self.df = self.df[[feature for feature in selected_features if feature in self.df.columns]]\n",
    "        return self\n",
    "\n",
    "    def split_data(self, train_start, train_end, val_start, val_end):\n",
    "        \"\"\"Splits dataset into train, validation, and test sets.\"\"\"\n",
    "        self.train_X, self.train_y, self.val_X, self.val_y, self.test_X, self.test_y = to_train_validation_test_data(\n",
    "            self.df, train_start, train_end, val_start, val_end\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def get_processed_data(self):\n",
    "        \"\"\"Returns processed train, validation, and test sets.\"\"\"\n",
    "        return self.train_X, self.train_y, self.val_X, self.val_y, self.test_X, self.test_y \n",
    "    \n",
    "class ExperimentMapper:\n",
    "\n",
    "    @staticmethod\n",
    "    def map_id_to_config(experiment_id: int):\n",
    "\n",
    "        config = []\n",
    "\n",
    "        if experiment_id  == 1:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-01-01\",\n",
    "                \"train_end\": \"2022-03-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-03-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        elif experiment_id  == 2:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-04-01\",\n",
    "                \"train_end\": \"2022-06-30\",\n",
    "                \"val_start\": \"2023-04-01\",\n",
    "                \"val_end\": \"2023-06-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        elif experiment_id  == 3:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-07-01\",\n",
    "                \"train_end\": \"2022-09-30\",\n",
    "                \"val_start\": \"2023-07-01\",\n",
    "                \"val_end\": \"2023-09-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 4:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-10-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 5:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-03-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 6:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-04-01\",\n",
    "                \"val_end\": \"2023-06-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 7:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-07-01\",\n",
    "                \"val_end\": \"2023-09-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 8:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-06-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 9:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-07-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 10:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-06-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 11:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-07-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id  == 12:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\", \"interval_index\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-06-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 13:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\", \"interval_index\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-07-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 14:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-01-01\",\n",
    "                \"train_end\": \"2022-03-31\",\n",
    "                \"val_start\": \"2023-04-01\",\n",
    "                \"val_end\": \"2023-06-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 15:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-07-01\",\n",
    "                \"train_end\": \"2022-09-30\",\n",
    "                \"val_start\": \"2023-04-01\",\n",
    "                \"val_end\": \"2023-06-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 16:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-01-01\",\n",
    "                \"train_end\": \"2022-03-31\",\n",
    "                \"val_start\": \"2023-10-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 17:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-01-01\",\n",
    "                \"train_end\": \"2022-03-31\",\n",
    "                \"val_start\": \"2023-07-01\",\n",
    "                \"val_end\": \"2023-09-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 18:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-04-01\",\n",
    "                \"train_end\": \"2022-06-30\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-03-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 19:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-04-01\",\n",
    "                \"train_end\": \"2022-06-30\",\n",
    "                \"val_start\": \"2023-07-01\",\n",
    "                \"val_end\": \"2023-09-30\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 20:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-04-01\",\n",
    "                \"train_end\": \"2022-06-30\",\n",
    "                \"val_start\": \"2023-10-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 21:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-07-01\",\n",
    "                \"train_end\": \"2022-09-30\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-03-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 22:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-07-01\",\n",
    "                \"train_end\": \"2022-09-30\",\n",
    "                \"val_start\": \"2023-10-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 23:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"power_t-96\"],\n",
    "                \"train_start\": \"2022-01-01\",\n",
    "                \"train_end\": \"2022-03-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-03-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        \n",
    "        elif experiment_id == 24:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        elif experiment_id == 25:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        elif experiment_id == 26:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 27:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 28:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\", \"interval_index\"],\n",
    "                \"train_start\": \"2022-10-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 29:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\", \"interval_index\"],\n",
    "                \"train_start\": \"2016-01-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 30:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\"],\n",
    "                \"train_start\": \"2016-01-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 31:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"power_t-96\"],\n",
    "                \"train_start\": \"2016-01-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        elif experiment_id == 32:\n",
    "            config = [\n",
    "                {\n",
    "                \"selected_features\": [\"ws_10m_loc_mean\", \"ws_100m_loc_mean\",\"power_t-96\"],\n",
    "                \"train_start\": \"2016-01-01\",\n",
    "                \"train_end\": \"2022-12-31\",\n",
    "                \"val_start\": \"2023-01-01\",\n",
    "                \"val_end\": \"2023-12-31\",\n",
    "                \"random_state\": 42\n",
    "                }\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(f\"Experiment ID {experiment_id} is not valid.\")\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "    @staticmethod\n",
    "    def get_experiment_ids_for_time_range(time_range: str):\n",
    "        # A method to filter experiment ids based on the time range.\n",
    "        # We're only interested in experiments that belong to Q4 2022\n",
    "\n",
    "        valid_ids = []\n",
    "        for experiment_id in range(1, 30):  # Assuming there are 29 experiments\n",
    "            config = ExperimentMapper.map_id_to_config(experiment_id)\n",
    "            for item in config:\n",
    "                if time_range in item[\"train_start\"] or time_range in item[\"val_start\"]:\n",
    "                    valid_ids.append(experiment_id)\n",
    "                    break\n",
    "        return valid_ids\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def extract_date_abbreviations_from_config(config):\n",
    "        \"\"\"\n",
    "        Processes training and validation date ranges into readable formats:\n",
    "        - Qx YEAR for quarters\n",
    "        - H1/H2 YEAR for half-years\n",
    "        - FY YEAR for full year\n",
    "        - YEAR-YEAR if the period spans multiple years\n",
    "        \"\"\"\n",
    "\n",
    "        def months_between(start_date_str, end_date_str):\n",
    "            start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "            end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "            months_diff = (end_date.year - start_date.year) * 12 + end_date.month - start_date.month\n",
    "            if end_date.day >= start_date.day:\n",
    "                months_diff += 1\n",
    "            return months_diff, start_date.year, end_date.year\n",
    "\n",
    "        def get_period(months_diff, start_date_str, start_year, end_year):\n",
    "            if start_year != end_year:\n",
    "                return f\"{start_year}-{end_year}\"\n",
    "\n",
    "            start_month = datetime.strptime(start_date_str, \"%Y-%m-%d\").month\n",
    "\n",
    "            if months_diff == 3:\n",
    "                quarter = (start_month - 1) // 3 + 1\n",
    "                return f\"Q{quarter} {start_year}\"\n",
    "            elif months_diff == 6:\n",
    "                return f\"H1 {start_year}\" if start_month <= 6 else f\"H2 {start_year}\"\n",
    "            elif months_diff == 12:\n",
    "                return f\"FY {start_year}\"\n",
    "            else:\n",
    "                return f\"{start_year}-{end_year}\"\n",
    "\n",
    "        # Extract dates\n",
    "        train_start = config[0][\"train_start\"]\n",
    "        train_end = config[0][\"train_end\"]\n",
    "        val_start = config[0][\"val_start\"]\n",
    "        val_end = config[0][\"val_end\"]\n",
    "\n",
    "        # Compute month diffs and periods\n",
    "        train_month_diff, train_start_year, train_end_year = months_between(train_start, train_end)\n",
    "        val_month_diff, val_start_year, val_end_year = months_between(val_start, val_end)\n",
    "\n",
    "        train_period = get_period(train_month_diff, train_start, train_start_year, train_end_year)\n",
    "        val_period = get_period(val_month_diff, val_start, val_start_year, val_end_year)\n",
    "\n",
    "        return f\"{train_period} / {val_period}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_feature_string_from_selected_features(config):\n",
    "\n",
    "        selected_features = config[0][\"selected_features\"]\n",
    "        \n",
    "        if isinstance(selected_features, str):\n",
    "            selected_features = [selected_features]  # Convert string to list\n",
    "\n",
    "        # Define the conditions and return the corresponding string\n",
    "        if set(selected_features) == {\"power_t-96\", \"ws_10m_loc_mean\", \"ws_100m_loc_mean\"}:\n",
    "            return \"power, mean ws\"\n",
    "        elif set(selected_features) == {\"power_t-96\"}:\n",
    "            return \"power\"\n",
    "        elif set(selected_features) == {\"power_t-96\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\"}:\n",
    "            return \"power, ws at 10 loc\"\n",
    "        elif set(selected_features) == {\"power_t-96\", \"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\"}:\n",
    "            return \"power, all ws\"\n",
    "        elif set(selected_features) == {\"power_t-96\", \"ws_10m_loc_mean\", \"ws_100m_loc_mean\", \"ws_10m_loc_1\", \"ws_10m_loc_2\", \"ws_10m_loc_3\", \"ws_10m_loc_4\", \"ws_10m_loc_5\", \"ws_10m_loc_6\", \"ws_10m_loc_7\", \"ws_10m_loc_8\", \"ws_10m_loc_9\", \"ws_10m_loc_10\", \n",
    "                                        \"ws_100m_loc_1\", \"ws_100m_loc_2\", \"ws_100m_loc_3\", \"ws_100m_loc_4\", \"ws_100m_loc_5\", \"ws_100m_loc_6\", \"ws_100m_loc_7\", \"ws_100m_loc_8\", \"ws_100m_loc_9\", \"ws_100m_loc_10\", \"interval_index\"}:\n",
    "            return \"power, all ws, time bin\"\n",
    "        else:\n",
    "            return \"Unknown features\"\n",
    "    \n",
    "@dataclass\n",
    "class Experiment_Baseline:\n",
    "    X_train: np.ndarray\n",
    "    y_train: np.ndarray\n",
    "    X_validation: np.ndarray\n",
    "    y_validation: np.ndarray\n",
    "    beta_0: None | float = None\n",
    "    beta_1: None | np.ndarray = None\n",
    "    sigma_sq: None | float = None\n",
    "    intercept: bool = True\n",
    "\n",
    "    def perform(self):\n",
    "        model = LinearRegression(fit_intercept=self.intercept)\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        if self.intercept:\n",
    "            self.beta_0 = model.intercept_  # Intercept (β₀) when fit_intercept=True\n",
    "        else:\n",
    "            self.beta_0 = 0.0  # Set beta_0 to 0 manually when fit_intercept=False\n",
    "\n",
    "        self.beta_1 = model.coef_    # Coefficient for P_t-96 (β₁)\n",
    "\n",
    "        # Calculate sigma^2 (variance of residuals)\n",
    "        y_pred = model.predict(self.X_train)\n",
    "        residuals = self.y_train - y_pred\n",
    "        self.sigma_sq = (residuals ** 2).sum() / (len(self.X_train) - 2)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def calculate_crps(self):\n",
    "        \"\"\"Calculates the Continuous Ranked Probability Score (CRPS).\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize CRPS statistics\n",
    "        crps_values = []\n",
    "        \n",
    "        crps_mean = 0\n",
    "        crps_min = 10000\n",
    "        crps_max = 0\n",
    "        counter = 0\n",
    "\n",
    "        for i, y in enumerate(self.y_validation):  # Iterate over the validation set\n",
    "            # Select the row corresponding to the i-th observation from X_validation\n",
    "            mu = self.beta_0 + np.dot(self.X_validation.iloc[i, :], self.beta_1)  # Use iloc for row selection\n",
    "            \n",
    "            sigma = np.sqrt(self.sigma_sq)  # Predicted standard deviation (using variance)\n",
    "            \n",
    "            # CDF and PDF of standard normal distribution\n",
    "            z = (y - mu) / sigma\n",
    "            \n",
    "            # Calculate PDF and CDF values for z\n",
    "            pdf_z = norm.pdf(z)  # Standard normal PDF at z\n",
    "            cdf_z = norm.cdf(z)  # Standard normal CDF at z\n",
    "\n",
    "            # CRPS formula for normal distribution\n",
    "            crps = sigma * (z * (2 * cdf_z - 1) + 2 * pdf_z - 1 / np.sqrt(np.pi))\n",
    "            crps_values.append(crps)\n",
    "            # Update CRPS statistics\n",
    "            #crps_mean += crps\n",
    "            #crps_min = min(crps_min, crps)\n",
    "            #crps_max = max(crps_max, crps)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # Print progress every 5000 iterations\n",
    "            if counter % 5000 == 0:\n",
    "                end_time = time.time()\n",
    "                elapsed = end_time - start_time\n",
    "                print(\"Elapsed time:\", elapsed)\n",
    "                print(\"Counter:\", counter)\n",
    "                start_time = time.time()\n",
    "        \n",
    "        # Calculate the average CRPS value\n",
    "        #crps_mean /= len(self.y_validation)\n",
    "        crps_mean = np.mean(crps_values)\n",
    "        crps_min = np.min(crps_values)\n",
    "        crps_max = np.max(crps_values)\n",
    "        crps_median = np.median(crps_values)\n",
    "        # Return the CRPS statistics\n",
    "        print(\"CRPS calculation finished\")\n",
    "        return crps_mean, crps_median, crps_min, crps_max\n",
    "\n",
    "\n",
    "    def calculate_nll(self):\n",
    "        \"\"\"Calculates the Negative Log-Likelihood (NLL).\"\"\"\n",
    "        start_time = time.time()\n",
    "        nll_values = []\n",
    "        counter = 0\n",
    "        sigma = np.sqrt(self.sigma_sq)  # Predicted standard deviation (using variance)\n",
    "        for i, y in enumerate(self.y_validation):  # Iterate over the validation set\n",
    "            # Select the row corresponding to the i-th observation from X_validation\n",
    "            mu = self.beta_0 + np.dot(self.X_validation.iloc[i, :], self.beta_1)  # Use iloc for row selection\n",
    "            counter += 1\n",
    "            # NLL formula for normal distribution\n",
    "            nll = 0.5 * np.log(2 * np.pi * sigma**2) + ((y - mu)**2) / (2 * sigma**2)\n",
    "            nll_values.append(nll)\n",
    "\n",
    "            if counter % 5000 == 0:\n",
    "                end_time = time.time()\n",
    "                elapsed = end_time - start_time\n",
    "                print(\"elapsed time\", elapsed)\n",
    "                start_time = time.time()\n",
    "                \n",
    "        nll_mean = np.mean(nll_values)\n",
    "        nll_min = np.min(nll_values)\n",
    "        nll_max = np.max(nll_values)\n",
    "        nll_median = np.median(nll_values)\n",
    "        return nll_mean, nll_median, nll_min, nll_max\n",
    "        \n",
    "\n",
    "class ExperimentStorage:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def save(self, experiment: Experiment_Baseline):\n",
    "        \"\"\"Save the ExperimentTracker object to a file.\"\"\"\n",
    "\n",
    "        directory = os.path.dirname(self.file_path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        with open(self.file_path, \"wb\") as f:\n",
    "            pickle.dump(experiment, f)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load the Experiment object from a file.\"\"\"\n",
    "        if os.path.exists(self.file_path):\n",
    "            with open(self.file_path, \"rb\") as f:\n",
    "                return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960cb142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "def run_baseline_model(experiment_ids, storage_path=\"experiments/\", fit_intercept=False):\n",
    "    \"\"\"\n",
    "    Run multiple experiments with different configurations.\n",
    "    \n",
    "    Parameters:\n",
    "    - experiment_configs (list of dict): Each dictionary should contain:\n",
    "        - selected_features\n",
    "        - train_start\n",
    "        - train_end\n",
    "        - val_start\n",
    "        - val_end\n",
    "        - random_state (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(experiment_ids, int):\n",
    "        experiment_ids = [experiment_ids]\n",
    "\n",
    "    experiment_mapper = ExperimentMapper()\n",
    "\n",
    "    for experiment_id in experiment_ids:\n",
    "\n",
    "        config_list = experiment_mapper.map_id_to_config(experiment_id)\n",
    "\n",
    "        for config in config_list:\n",
    "\n",
    "            print(f\"Running experiment {experiment_id}...\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Extract experiment-specific parameters\n",
    "            selected_features = config[\"selected_features\"]\n",
    "            train_start = config[\"train_start\"]\n",
    "            train_end = config[\"train_end\"]\n",
    "            val_start = config[\"val_start\"]\n",
    "            val_end = config[\"val_end\"]\n",
    "            random_state = config.get(\"random_state\", 42)  # Default random state if not provided\n",
    "\n",
    "            # Preprocess the data\n",
    "            print(\"- Preprocessing data...\")\n",
    "            preprocessor = DataPreprocessor()\n",
    "            preprocessor.load_data()\n",
    "            preprocessor.transform_power()\n",
    "            preprocessor.add_interval_index()\n",
    "            preprocessor.add_lagged_features()\n",
    "            preprocessor.prepare_features(selected_features)\n",
    "            print(\"- Splitting data into train, validation, test...\")\n",
    "            preprocessor.split_data(train_start, train_end, val_start, val_end)\n",
    "            train_X, train_y, validation_X, validation_y, test_X, test_y = preprocessor.get_processed_data()\n",
    "            display(train_X.head(3))\n",
    "\n",
    "\n",
    "            print(\"- Running model training\")\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "                experiment = Experiment_Baseline(X_train=train_X, y_train=train_y, X_validation=validation_X, y_validation=validation_y, intercept=fit_intercept)\n",
    "                experiment = experiment.perform()\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            print(f\"⏱️ Experiment {experiment_id} completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "            print(\"- Saving experiment results...\")\n",
    "            experiment_filename = f\"{storage_path}/experiment_{experiment_id}.pkl\"\n",
    "            storage = ExperimentStorage(experiment_filename)\n",
    "            storage.save(experiment)\n",
    "            print(f\"Experiment saved to: {experiment_filename}\")\n",
    "\n",
    "    print(\"All experiments completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faaa2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_scores_baseline(experiment_id, storage_path=\"experiments/\", all_scores=False):\n",
    "    \n",
    "    experiment_filename = f\"{storage_path}/experiment_{experiment_id}.pkl\"\n",
    "    storage = ExperimentStorage(experiment_filename)\n",
    "    experiment = storage.load()\n",
    "\n",
    "    crps_mean, crps_median, crps_min, crps_max = experiment.calculate_crps()\n",
    "    mean_nll, median_nll, min_nll, max_nll  = experiment.calculate_nll()\n",
    "\n",
    "    scores = {\n",
    "        'Metric': ['nll', 'crps'],\n",
    "        'Mean': [mean_nll, crps_mean],\n",
    "        'Median': [median_nll, crps_median],\n",
    "        'Min': [min_nll, crps_min],\n",
    "        'Max': [max_nll, crps_max]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(scores)\n",
    "\n",
    "\n",
    "    file_name = f\"{storage_path}/experiment_results_{experiment_id}.pkl\"\n",
    "    storage = ExperimentStorage(file_name)\n",
    "    storage.save(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8b26af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment 32...\n",
      "- Preprocessing data...\n",
      "- Splitting data into train, validation, test...\n",
      "# of training observations: 245376 | 77.76%\n",
      "# of validation observations: 35040 | 11.10%\n",
      "# of test observations: 35133 | 11.13%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ws_10m_loc_mean</th>\n",
       "      <th>ws_100m_loc_mean</th>\n",
       "      <th>power_t-96</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 00:00:00</th>\n",
       "      <td>2.830</td>\n",
       "      <td>5.19</td>\n",
       "      <td>-2.465104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 00:15:00</th>\n",
       "      <td>2.825</td>\n",
       "      <td>5.16</td>\n",
       "      <td>-2.499602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 00:30:00</th>\n",
       "      <td>2.820</td>\n",
       "      <td>5.13</td>\n",
       "      <td>-2.485377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ws_10m_loc_mean  ws_100m_loc_mean  power_t-96\n",
       "time                                                              \n",
       "2016-01-02 00:00:00            2.830              5.19   -2.465104\n",
       "2016-01-02 00:15:00            2.825              5.16   -2.499602\n",
       "2016-01-02 00:30:00            2.820              5.13   -2.485377"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Running model training\n",
      "⏱️ Experiment 32 completed in 2.92 seconds\n",
      "- Saving experiment results...\n",
      "Experiment saved to: C:/Users/Minu/Documents/Baseline/experiments/experiment_32.pkl\n",
      "All experiments completed and saved.\n"
     ]
    }
   ],
   "source": [
    "id = 32\n",
    "run_baseline_model([id], \"C:/Users/Minu/Documents/Baseline/experiments\", fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c72fd0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.8329226970672607\n",
      "Counter: 5000\n",
      "Elapsed time: 0.8288474082946777\n",
      "Counter: 10000\n",
      "Elapsed time: 0.8326573371887207\n",
      "Counter: 15000\n",
      "Elapsed time: 0.8185572624206543\n",
      "Counter: 20000\n",
      "Elapsed time: 0.8296365737915039\n",
      "Counter: 25000\n",
      "Elapsed time: 0.8348667621612549\n",
      "Counter: 30000\n",
      "Elapsed time: 0.8325257301330566\n",
      "Counter: 35000\n",
      "CRPS calculation finished\n",
      "elapsed time 0.25159668922424316\n",
      "elapsed time 0.2624778747558594\n",
      "elapsed time 0.2553431987762451\n",
      "elapsed time 0.26299452781677246\n",
      "elapsed time 0.2705802917480469\n",
      "elapsed time 0.2640252113342285\n",
      "elapsed time 0.2775447368621826\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nll</td>\n",
       "      <td>0.875028</td>\n",
       "      <td>0.510165</td>\n",
       "      <td>0.305783</td>\n",
       "      <td>21.421837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crps</td>\n",
       "      <td>0.315546</td>\n",
       "      <td>0.212015</td>\n",
       "      <td>0.126578</td>\n",
       "      <td>3.214322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metric      Mean    Median       Min        Max\n",
       "0    nll  0.875028  0.510165  0.305783  21.421837\n",
       "1   crps  0.315546  0.212015  0.126578   3.214322"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_scores_baseline(32, \"C:/Users/Minu/Documents/Baseline/experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820da21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'selected_features': ['power_t-96'], 'train_start': '2022-10-01', 'train_end': '2022-12-31', 'val_start': '2023-01-01', 'val_end': '2023-12-31', 'random_state': 42}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ExperimentMapper.extract_date_abbreviations_from_config() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     config \u001b[38;5;241m=\u001b[39m experiment_mapper_t\u001b[38;5;241m.\u001b[39mmap_id_to_config(i)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mexperiment_mapper_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_date_abbreviations_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create dataframes with extra columns\u001b[39;00m\n\u001b[0;32m     17\u001b[0m nll_means_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(nll_mean, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: ExperimentMapper.extract_date_abbreviations_from_config() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "nll_mean = []\n",
    "crps_mean = []\n",
    "\n",
    "experiment_mapper_t = ExperimentMapper()\n",
    "for i in range(24, 30):\n",
    "    file = f\"C:/Users/Minu/Documents/Baseline/experiments/experiment_results_{i}.pkl\"\n",
    "\n",
    "    with open(file, \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "        nll_mean.append((df['Metric'].iloc[0], df['Mean'].iloc[0]))\n",
    "        crps_mean.append((df['Metric'].iloc[1], df['Mean'].iloc[1]))\n",
    "    \n",
    "# Create dataframes with extra columns\n",
    "nll_means_df = pd.DataFrame(nll_mean, columns=['Metric', 'Mean'])\n",
    "crps_means_df = pd.DataFrame(crps_mean, columns=['Metric', 'Mean'])\n",
    "\n",
    "# Set index to start at 1\n",
    "nll_means_df.index = range(24, 30)\n",
    "crps_means_df.index = range(24, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d41522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>nll</td>\n",
       "      <td>1.501098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nll</td>\n",
       "      <td>0.926073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nll</td>\n",
       "      <td>0.925112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nll</td>\n",
       "      <td>0.923680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>nll</td>\n",
       "      <td>0.924472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>nll</td>\n",
       "      <td>0.731674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metric      Mean\n",
       "24    nll  1.501098\n",
       "25    nll  0.926073\n",
       "26    nll  0.925112\n",
       "27    nll  0.923680\n",
       "28    nll  0.924472\n",
       "29    nll  0.731674"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c1ca8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>crps</td>\n",
       "      <td>0.594598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>crps</td>\n",
       "      <td>0.312445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>crps</td>\n",
       "      <td>0.281502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>crps</td>\n",
       "      <td>0.280219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>crps</td>\n",
       "      <td>0.280346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>crps</td>\n",
       "      <td>0.273208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Metric      Mean\n",
       "24   crps  0.594598\n",
       "25   crps  0.312445\n",
       "26   crps  0.281502\n",
       "27   crps  0.280219\n",
       "28   crps  0.280346\n",
       "29   crps  0.273208"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps_means_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57a4ce",
   "metadata": {},
   "source": [
    "## log(P(t)/ P_max) = -3.4 - 0.10 * ws_10 + 0.34 * ws_100 + 0.14 * log(P(t-96) / P_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "19193f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta_0: -1.062821888023514\n",
      "Estimated beta_1: 0.43036274221059917\n",
      "Estimated sigma: 1.0606888484543233\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "y = exp.y_validation.values\n",
    "# Define the log-likelihood function for the model\n",
    "def log_likelihood(params, y, lag=96):\n",
    "    beta_0, beta_1, sigma = params\n",
    "    \n",
    "    # Compute the residuals (errors)\n",
    "    residuals = y[lag:] - beta_0 - beta_1 * y[lag-96:-96]\n",
    "    \n",
    "    # Log-likelihood for normal errors\n",
    "    log_likelihood_value = -len(residuals)/2 * np.log(2 * np.pi * sigma**2) - np.sum(residuals**2) / (2 * sigma**2)\n",
    "    \n",
    "    return -log_likelihood_value  # Return negative log-likelihood for minimization\n",
    "\n",
    "# Initial guess for parameters: beta_0 = 0, beta_1 = 0, sigma = 1\n",
    "initial_guess = [0, 0, 1]\n",
    "\n",
    "# Perform the optimization to maximize the log-likelihood\n",
    "result = minimize(log_likelihood, initial_guess, args=(y), bounds=[(None, None), (None, None), (0.001, None)])\n",
    "\n",
    "# Extract the estimated parameters\n",
    "beta_0_est, beta_1_est, sigma_est = result.x\n",
    "\n",
    "# Output the results\n",
    "print(\"Estimated beta_0:\", beta_0_est)\n",
    "print(\"Estimated beta_1:\", beta_1_est)\n",
    "print(\"Estimated sigma:\", sigma_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee63039",
   "metadata": {},
   "source": [
    "## Compare Baseline model with TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa996f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_3(method=\"tabpfn\"):\n",
    "\n",
    "    if method == \"tabpfn\":\n",
    "        dir = \"C:/Users/Minu/Documents/TabPFN/experiments\"\n",
    "        nll_name = \"nll_5000\"\n",
    "        crps_name = \"crps_5000\"\n",
    "    else:\n",
    "        dir = \"C:/Users/Minu/Documents/Baseline/experiments\"\n",
    "        nll_name = \"nll\"\n",
    "        crps_name = \"crps\"\n",
    "\n",
    "    # Step 1: Load experiment files\n",
    "\n",
    "    files = glob.glob(os.path.join(dir, 'experiment_results_*'))\n",
    "\n",
    "    ids = [int(re.search(r'\\d+', s).group()) for s in files if re.search(r'\\d+', s)]\n",
    "\n",
    "    # Step 2: Define feature categories to track\n",
    "    feature_groups = {\n",
    "        \"power, all ws\": [],\n",
    "        \"power, all ws, time bin\": [],\n",
    "        'power, mean ws': [],\n",
    "        \"power, ws at 10 loc\": []\n",
    "    }\n",
    "\n",
    "    # Step 3: Filter and categorize IDs\n",
    "    for id in ids:\n",
    "        config = ExperimentMapper.map_id_to_config(id)\n",
    "        dates = ExperimentMapper.extract_date_abbreviations_from_config(config)\n",
    "        \n",
    "        # Clean and split date string\n",
    "        dates_split = dates.strip().split(\" / \")\n",
    "        if not dates_split or len(dates_split) < 2:\n",
    "            continue\n",
    "\n",
    "        if dates.startswith(\"Q4\"):\n",
    "            feature = ExperimentMapper.get_feature_string_from_selected_features(config)\n",
    "            feature = feature.strip()\n",
    "            if feature in feature_groups:\n",
    "                feature_groups[feature].append(id)\n",
    "        \n",
    "\n",
    "    # Step 4: Load metrics (CRPS, NLL) per feature group\n",
    "    results_summary = {}\n",
    "\n",
    "    for feature_name, id_list in feature_groups.items():\n",
    "        nlls = []\n",
    "        crps = []\n",
    "        for id in id_list:\n",
    "            file_path = os.path.join(dir, f\"experiment_results_{id}.pkl\")\n",
    "            if not os.path.exists(file_path):\n",
    "                continue\n",
    "\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                try:\n",
    "                    nll = data.loc[data['Metric'] == nll_name, 'Mean'].values[0]\n",
    "                    crps_val = data.loc[data['Metric'] == crps_name, 'Mean'].values[0]\n",
    "                    nlls.append(nll)\n",
    "                    crps.append(crps_val)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "\n",
    "        # Calculate and store averages\n",
    "        if nlls and crps:\n",
    "            results_summary[feature_name] = {\n",
    "                \"average_nll\": round(sum(nlls) / len(nlls), 5),\n",
    "                \"average_crps\": round(sum(crps) / len(crps), 5),\n",
    "                \"count\": len(nlls)\n",
    "            }\n",
    "\n",
    "\n",
    "    # Rename features for presentation\n",
    "    pretty_names = {\n",
    "        \"power, mean ws\": \"Power, mean wind speed\",\n",
    "        \"power, ws at 10 loc\": \"Power, 10 wind speeds\",\n",
    "        \"power, all ws\": \"Power, all wind speeds\",\n",
    "        \"power, all ws, time bin\": \"Power, all wind speeds, time\"\n",
    "    }\n",
    "\n",
    "    # Build DataFrame\n",
    "    df_data = []\n",
    "    for feature, stats in results_summary.items():\n",
    "        df_data.append({\n",
    "            \"Feature model\": pretty_names.get(feature, feature),\n",
    "            \"CRPS\": stats[\"average_crps\"],\n",
    "            \"NLL\": stats[\"average_nll\"]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "    df = df[[\"Feature model\", \"CRPS\", \"NLL\"]]  # Reorder columns\n",
    "    df = df.set_index(\"Feature model\")\n",
    "    df = df.round(3)\n",
    "    \n",
    "    feature_order = [\n",
    "        \"Power, mean wind speed\",\n",
    "        \"Power, 10 wind speeds\",\n",
    "        \"Power, all wind speeds\",\n",
    "        \"Power, all wind speeds, time\"\n",
    "    ]\n",
    "    df.index = pd.CategoricalIndex(df.index, categories=feature_order, ordered=True)\n",
    "    df = df.sort_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2f22bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_3(method=\"tabpfn\"):\n",
    "\n",
    "    if method == \"tabpfn\":\n",
    "        dir = \"C:/Users/Minu/Documents/TabPFN/experiments\"\n",
    "        nll_name = \"nll_5000\"\n",
    "        crps_name = \"crps_5000\"\n",
    "        compute_2016 = False\n",
    "    else:\n",
    "        dir = \"C:/Users/Minu/Documents/Baseline/experiments\"\n",
    "        nll_name = \"nll\"\n",
    "        crps_name = \"crps\"\n",
    "        compute_2016 = True\n",
    "\n",
    "    # Step 1: Load experiment files\n",
    "\n",
    "    def categorize_ID_to_feature_group():\n",
    "        files = glob.glob(os.path.join(dir, 'experiment_results_*'))\n",
    "        ids = [int(re.search(r'\\d+', s).group()) for s in files if re.search(r'\\d+', s)]\n",
    "\n",
    "        feature_groups = {\n",
    "            \"power, all ws\": [],\n",
    "            \"power, all ws, time bin\": [],\n",
    "            'power, mean ws': [],\n",
    "            \"power, ws at 10 loc\": []\n",
    "        }\n",
    "\n",
    "        feature_groups_2016 = {\n",
    "            \"power, all ws\": [],\n",
    "            \"power, all ws, time bin\": [],\n",
    "            'power, mean ws': [],\n",
    "            \"power, ws at 10 loc\": []\n",
    "        } if compute_2016 else None\n",
    "\n",
    "        for id in ids:\n",
    "            config = ExperimentMapper.map_id_to_config(id)\n",
    "            dates = ExperimentMapper.extract_date_abbreviations_from_config(config)\n",
    "            dates_split = dates.strip().split(\" / \")\n",
    "            if not dates_split or len(dates_split) < 2:\n",
    "                continue\n",
    "\n",
    "            feature = ExperimentMapper.get_feature_string_from_selected_features(config).strip()\n",
    "            if feature not in feature_groups:\n",
    "                continue\n",
    "\n",
    "            # Add to overall group if it's Q4 (original behavior)\n",
    "            if dates.startswith(\"Q4\"):\n",
    "                feature_groups[feature].append(id)\n",
    "\n",
    "            # Add to 2016 group if any date starts with 2016\n",
    "            if dates.startswith(\"2016\"):\n",
    "                feature_groups_2016[feature].append(id)\n",
    "\n",
    "        return feature_groups, feature_groups_2016\n",
    "\n",
    "\n",
    "        \n",
    "    def load_metrics_per_feature_group(feature_groups, feature_groups_2016):\n",
    "        results_summary = {}\n",
    "\n",
    "        for feature_name in feature_groups:\n",
    "            id_list_all = feature_groups[feature_name]\n",
    "            id_list_2016 = feature_groups_2016[feature_name] if feature_groups_2016 else []\n",
    "\n",
    "            def compute_metrics(id_list):\n",
    "                nlls = []\n",
    "                crps = []\n",
    "                for id in id_list:\n",
    "                    file_path = os.path.join(dir, f\"experiment_results_{id}.pkl\")\n",
    "                    if not os.path.exists(file_path):\n",
    "                        continue\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                        try:\n",
    "                            nll = data.loc[data['Metric'] == nll_name, 'Mean'].values[0]\n",
    "                            crps_val = data.loc[data['Metric'] == crps_name, 'Mean'].values[0]\n",
    "                            nlls.append(nll)\n",
    "                            crps.append(crps_val)\n",
    "                        except IndexError:\n",
    "                            continue\n",
    "                return nlls, crps\n",
    "\n",
    "            nlls_all, crps_all = compute_metrics(id_list_all)\n",
    "            nlls_2016, crps_2016 = compute_metrics(id_list_2016) if compute_2016 else ([], [])\n",
    "\n",
    "            if nlls_all and crps_all:\n",
    "                results_summary[feature_name] = {\n",
    "                    \"average_nll\": round(sum(nlls_all) / len(nlls_all), 5),\n",
    "                    \"average_crps\": round(sum(crps_all) / len(crps_all), 5),\n",
    "                    \"average_nll_2016\": round(sum(nlls_2016) / len(nlls_2016), 5) if nlls_2016 else None if nlls_2016 else None,\n",
    "                    \"average_crps_2016\": round(sum(crps_2016) / len(crps_2016), 5) if crps_2016 else None if nlls_2016 else None,\n",
    "                    \"count\": len(nlls_all),\n",
    "                    \"count_2016\": len(nlls_2016) if nlls_2016 else None\n",
    "                }\n",
    "        return results_summary\n",
    "\n",
    "\n",
    "    def load_result_df(results_summary):\n",
    "        pretty_names = {\n",
    "            \"power, mean ws\": \"Power, mean wind speed\",\n",
    "            \"power, ws at 10 loc\": \"Power, 10 wind speeds\",\n",
    "            \"power, all ws\": \"Power, all wind speeds\",\n",
    "            \"power, all ws, time bin\": \"Power, all wind speeds, time\"\n",
    "        }\n",
    "\n",
    "        df_data = []\n",
    "        for feature, stats in results_summary.items():\n",
    "            row = {\n",
    "                \"Feature model\": pretty_names.get(feature, feature),\n",
    "                \"CRPS\": stats[\"average_crps\"],\n",
    "                \"NLL\": stats[\"average_nll\"],\n",
    "            }\n",
    "\n",
    "            if compute_2016:\n",
    "                row[\"CRPS_2016\"] = stats[\"average_crps_2016\"]\n",
    "                row[\"NLL_2016\"] = stats[\"average_nll_2016\"]\n",
    "\n",
    "            df_data.append(row)\n",
    "\n",
    "        df = pd.DataFrame(df_data)\n",
    "        columns = [\"Feature model\", \"CRPS\", \"NLL\"]\n",
    "        if compute_2016:\n",
    "            columns += [\"CRPS_2016\", \"NLL_2016\"]\n",
    "\n",
    "        df = df[columns]\n",
    "        df = df.set_index(\"Feature model\")\n",
    "        df = df.round(3)\n",
    "\n",
    "        feature_order = [\n",
    "            \"Power, mean wind speed\",\n",
    "            \"Power, 10 wind speeds\",\n",
    "            \"Power, all wind speeds\",\n",
    "            \"Power, all wind speeds, time\"\n",
    "        ]\n",
    "        df.index = pd.CategoricalIndex(df.index, categories=feature_order, ordered=True)\n",
    "        df = df.sort_index()\n",
    "        return df\n",
    "    \n",
    "    feature_groups, feature_groups_2016 = categorize_ID_to_feature_group()\n",
    "    results_summary = load_metrics_per_feature_group(feature_groups, feature_groups_2016)\n",
    "    df = load_result_df(results_summary)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0cc2b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRPS</th>\n",
       "      <th>NLL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Power, mean wind speed</th>\n",
       "      <td>0.343</td>\n",
       "      <td>1.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, 10 wind speeds</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds</th>\n",
       "      <td>0.204</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds, time</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               CRPS    NLL\n",
       "Feature model                             \n",
       "Power, mean wind speed        0.343  1.774\n",
       "Power, 10 wind speeds         0.203  0.723\n",
       "Power, all wind speeds        0.204  0.720\n",
       "Power, all wind speeds, time  0.200  0.699"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = analyze_3(\"tabpfn\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b20912a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRPS</th>\n",
       "      <th>NLL</th>\n",
       "      <th>CRPS_2016</th>\n",
       "      <th>NLL_2016</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Power, mean wind speed</th>\n",
       "      <td>0.312</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, 10 wind speeds</th>\n",
       "      <td>0.282</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds, time</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               CRPS    NLL  CRPS_2016  NLL_2016\n",
       "Feature model                                                  \n",
       "Power, mean wind speed        0.312  0.926      0.316     0.875\n",
       "Power, 10 wind speeds         0.282  0.925      0.274     0.735\n",
       "Power, all wind speeds        0.280  0.924      0.274     0.733\n",
       "Power, all wind speeds, time  0.280  0.924      0.273     0.732"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = analyze_3(\"baseline\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "663fd627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRPS</th>\n",
       "      <th>NLL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Power, mean wind speed</th>\n",
       "      <td>0.343</td>\n",
       "      <td>1.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, 10 wind speeds</th>\n",
       "      <td>0.203</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds</th>\n",
       "      <td>0.204</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds, time</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               CRPS    NLL\n",
       "Feature model                             \n",
       "Power, mean wind speed        0.343  1.774\n",
       "Power, 10 wind speeds         0.203  0.723\n",
       "Power, all wind speeds        0.204  0.720\n",
       "Power, all wind speeds, time  0.200  0.699"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = analyze_3(\"tabpfn\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc17d50",
   "metadata": {},
   "source": [
    "## Function to extract CRPS, NLL matrix for different features from NGBoost (C:\\Users\\Minu\\Documents\\NGboost\\q4_train\\caseXX + Merged_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0149821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_3_ngboost(filepath):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(filepath)\n",
    "\n",
    "    # Ensure 'loss_function' is treated as string\n",
    "    df['loss_function'] = df['loss_function'].astype(str)\n",
    "\n",
    "    # List of feature_abbr values we're interested in\n",
    "    target_features = {\n",
    "        \"p, ws_mean, ws_10_loc\",\n",
    "        \"p, ws_mean, ws_10_loc, t_index\",\n",
    "        \"p, ws_mean\",\n",
    "        \"p, ws_10_loc\"\n",
    "    }\n",
    "\n",
    "    # Filter rows\n",
    "    filtered = df[\n",
    "        df['loss_function'].str.contains('LogScore', case=False, na=False) &\n",
    "        df['feature_abbr'].isin(target_features)\n",
    "    ]\n",
    "\n",
    "    # Rename the feature_abbr values\n",
    "    rename_map = {\n",
    "        \"p, ws_10_loc\": \"Power, 10 wind speeds\",\n",
    "        \"p, ws_mean\": \"Power, mean wind speed\",\n",
    "        \"p, ws_mean, ws_10_loc, t_index\": \"Power, all wind speeds, time\",\n",
    "        \"p, ws_mean, ws_10_loc\": \"Power, all wind speeds\"\n",
    "    }\n",
    "    filtered['feature_abbr'] = filtered['feature_abbr'].map(rename_map)\n",
    "\n",
    "    # Extract only the required columns and include renamed feature_abbr\n",
    "    result = filtered[['feature_abbr', 'CRPS_gaussian_mean', 'NLL_mean']]\n",
    "\n",
    "     # Custom sort\n",
    "    feature_order = [\n",
    "        \"Power, mean wind speed\",\n",
    "        \"Power, 10 wind speeds\",\n",
    "        \"Power, all wind speeds\",\n",
    "        \"Power, all wind speeds, time\"\n",
    "    ]\n",
    "    result['feature_abbr'] = pd.Categorical(result['feature_abbr'], categories=feature_order, ordered=True)\n",
    "    result = result.sort_values('feature_abbr').reset_index(drop=True)\n",
    "    result = result.round(3)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd393501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered['feature_abbr'] = filtered['feature_abbr'].map(rename_map)\n",
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['feature_abbr'] = pd.Categorical(result['feature_abbr'], categories=feature_order, ordered=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de6ab25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered['feature_abbr'] = filtered['feature_abbr'].map(rename_map)\n",
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['feature_abbr'] = pd.Categorical(result['feature_abbr'], categories=feature_order, ordered=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_abbr</th>\n",
       "      <th>CRPS_gaussian_mean</th>\n",
       "      <th>NLL_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Power, mean wind speed</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Power, 10 wind speeds</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Power, all wind speeds</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Power, all wind speeds, time</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature_abbr  CRPS_gaussian_mean  NLL_mean\n",
       "0        Power, mean wind speed               0.284     0.737\n",
       "1         Power, 10 wind speeds               0.196     0.357\n",
       "2        Power, all wind speeds               0.196     0.353\n",
       "3  Power, all wind speeds, time               0.194     0.340"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngboost_matrix = analyze_3_ngboost(\"C:/Users/Minu/Documents/NGboost/q4_train/Merged_sheet.xlsx\")\n",
    "ngboost_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c68000f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered['feature_abbr'] = filtered['feature_abbr'].map(rename_map)\n",
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['feature_abbr'] = pd.Categorical(result['feature_abbr'], categories=feature_order, ordered=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_abbr</th>\n",
       "      <th>CRPS_gaussian_mean</th>\n",
       "      <th>NLL_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Power, mean wind speed</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Power, 10 wind speeds</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Power, all wind speeds</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Power, all wind speeds, time</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature_abbr  CRPS_gaussian_mean  NLL_mean\n",
       "0        Power, mean wind speed               0.270     0.544\n",
       "1         Power, 10 wind speeds               0.182     0.171\n",
       "2        Power, all wind speeds               0.183     0.174\n",
       "3  Power, all wind speeds, time               0.181     0.169"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_3_ngboost(\"C:/Users/Minu/Documents/NGboost/full_year/Merged_sheet.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9703e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_table_q4():\n",
    "    # Load all results\n",
    "    ngboost_matrix_q4 = analyze_3_ngboost(\"C:/Users/Minu/Documents/NGboost/q4_train/Merged_sheet.xlsx\")\n",
    "    ngboost_matrix_ft = analyze_3_ngboost(\"C:/Users/Minu/Documents/NGboost/full_year/Merged_sheet.xlsx\")\n",
    "    tabpfn_result_q4 = analyze_3(\"tabpfn\")\n",
    "    baseline_result_q4 = analyze_3(\"baseline\")\n",
    "\n",
    "    # Prepare CRPS\n",
    "    ngboost_crps_q4 = ngboost_matrix_q4.set_index('feature_abbr')[['CRPS_gaussian_mean']].rename(columns={'CRPS_gaussian_mean': 'NGBoost (Q4 training)'})\n",
    "    ngboost_crps_ft = ngboost_matrix_ft.set_index('feature_abbr')[['CRPS_gaussian_mean']].rename(columns={'CRPS_gaussian_mean': 'NGBoost (full training)'})\n",
    "    \n",
    "    tabpfn_crps_q4 = tabpfn_result_q4[['CRPS']].rename(columns={'CRPS': 'TabPFN (Q4 training)'})\n",
    "    baseline_crps_q4 = baseline_result_q4[['CRPS']].rename(columns={'CRPS': 'Baseline (Q4 training)'})\n",
    "    baseline_crps_ft = baseline_result_q4[['CRPS_2016']].rename(columns={'CRPS_2016': 'Baseline (full training)'})\n",
    "\n",
    "    crps_matrix = pd.concat([baseline_crps_q4, baseline_crps_ft, ngboost_crps_q4, ngboost_crps_ft, tabpfn_crps_q4], axis=1)\n",
    "\n",
    "    # Prepare NLL\n",
    "    ngboost_nll_q4 = ngboost_matrix_q4.set_index('feature_abbr')[['NLL_mean']].rename(columns={'NLL_mean': 'NGBoost (Q4 training)'})\n",
    "    ngboost_nll_ft = ngboost_matrix_ft.set_index('feature_abbr')[['NLL_mean']].rename(columns={'NLL_mean': 'NGBoost (full training)'})\n",
    "\n",
    "    tabpfn_nll_q4 = tabpfn_result_q4[['NLL']].rename(columns={'NLL': 'TabPFN (Q4 training)'})\n",
    "    baseline_nll_q4 = baseline_result_q4[['NLL']].rename(columns={'NLL': 'Baseline (Q4 training)'})\n",
    "    baseline_nll_ft = baseline_result_q4[['NLL_2016']].rename(columns={'NLL_2016': 'Baseline (full training)'})\n",
    "\n",
    "    nll_matrix = pd.concat([baseline_nll_q4, baseline_nll_ft, ngboost_nll_q4, ngboost_nll_ft, tabpfn_nll_q4], axis=1)\n",
    "\n",
    "    return crps_matrix.round(3), nll_matrix.round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb34fb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered['feature_abbr'] = filtered['feature_abbr'].map(rename_map)\n",
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['feature_abbr'] = pd.Categorical(result['feature_abbr'], categories=feature_order, ordered=True)\n",
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered['feature_abbr'] = filtered['feature_abbr'].map(rename_map)\n",
      "C:\\Users\\Minu\\AppData\\Local\\Temp\\ipykernel_12272\\1181083803.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['feature_abbr'] = pd.Categorical(result['feature_abbr'], categories=feature_order, ordered=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline (Q4 training)</th>\n",
       "      <th>Baseline (full training)</th>\n",
       "      <th>NGBoost (Q4 training)</th>\n",
       "      <th>NGBoost (full training)</th>\n",
       "      <th>TabPFN (Q4 training)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Power, mean wind speed</th>\n",
       "      <td>0.312</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, 10 wind speeds</th>\n",
       "      <td>0.282</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds, time</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Baseline (Q4 training)  \\\n",
       "Power, mean wind speed                         0.312   \n",
       "Power, 10 wind speeds                          0.282   \n",
       "Power, all wind speeds                         0.280   \n",
       "Power, all wind speeds, time                   0.280   \n",
       "\n",
       "                              Baseline (full training)  NGBoost (Q4 training)  \\\n",
       "Power, mean wind speed                           0.316                  0.284   \n",
       "Power, 10 wind speeds                            0.274                  0.196   \n",
       "Power, all wind speeds                           0.274                  0.196   \n",
       "Power, all wind speeds, time                     0.273                  0.194   \n",
       "\n",
       "                              NGBoost (full training)  TabPFN (Q4 training)  \n",
       "Power, mean wind speed                          0.270                 0.343  \n",
       "Power, 10 wind speeds                           0.182                 0.203  \n",
       "Power, all wind speeds                          0.183                 0.204  \n",
       "Power, all wind speeds, time                    0.181                 0.200  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crps_matrix, nll_matrix = final_table_q4()\n",
    "crps_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a421287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline (Q4 training)</th>\n",
       "      <th>Baseline (full training)</th>\n",
       "      <th>NGBoost (Q4 training)</th>\n",
       "      <th>NGBoost (full training)</th>\n",
       "      <th>TabPFN (Q4 training)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Power, mean wind speed</th>\n",
       "      <td>0.926</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.544</td>\n",
       "      <td>1.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, 10 wind speeds</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds</th>\n",
       "      <td>0.924</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power, all wind speeds, time</th>\n",
       "      <td>0.924</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Baseline (Q4 training)  \\\n",
       "Power, mean wind speed                         0.926   \n",
       "Power, 10 wind speeds                          0.925   \n",
       "Power, all wind speeds                         0.924   \n",
       "Power, all wind speeds, time                   0.924   \n",
       "\n",
       "                              Baseline (full training)  NGBoost (Q4 training)  \\\n",
       "Power, mean wind speed                           0.875                  0.737   \n",
       "Power, 10 wind speeds                            0.735                  0.357   \n",
       "Power, all wind speeds                           0.733                  0.353   \n",
       "Power, all wind speeds, time                     0.732                  0.340   \n",
       "\n",
       "                              NGBoost (full training)  TabPFN (Q4 training)  \n",
       "Power, mean wind speed                          0.544                 1.774  \n",
       "Power, 10 wind speeds                           0.171                 0.723  \n",
       "Power, all wind speeds                          0.174                 0.720  \n",
       "Power, all wind speeds, time                    0.169                 0.699  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd843e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "LaTeX document has been saved to: C:/Users/Minu/OneDrive/Arbeit/HTWG/Master/Masterarbeit/thesis_teamprojekt_templates-master/chapters/overall_comparison_of_crps_nll.tex\n"
     ]
    }
   ],
   "source": [
    "from analysis.latex import export_final_comparison_to_latex\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "export_final_comparison_to_latex(crps_matrix, nll_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
