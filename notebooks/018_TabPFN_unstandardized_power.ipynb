{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from analysis.datasets import load_entsoe\n",
    "from analysis.splits import to_train_validation_test_data\n",
    "from analysis.transformations import scale_power_data\n",
    "from tabpfn import TabPFNRegressor\n",
    "from analysis.transformations import add_interval_index, add_lagged_features\n",
    "from torchinfo import summary\n",
    "from analysis.TabPFN_copy import evaluate\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from analysis.TabPFN_copy import fit_tail_distribution, plot_cdf_pdf_dynamic, plot_pdf_from_logits\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_crps_pytorch(logits, bin_edges, y_values):\n",
    "    \"\"\"\n",
    "    Computes the CRPS for multiple rows of logits and corresponding y-values using PyTorch.\n",
    "\n",
    "    Args:\n",
    "        logits: Tensor of shape (N, 5000) - unnormalized logits for each row.\n",
    "        bin_edges: Tensor of shape (5001,) - common bin edges for all rows.\n",
    "        y_values: Tensor of shape (N,) - target values for each row.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (N,) containing the CRPS values for each row.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = torch.softmax(logits, dim=1)  # (N, 5000)\n",
    "\n",
    "    # Compute CDF (cumulative sum of probabilities)\n",
    "    cdf = torch.cumsum(probs, dim=1)  # (N, 5000)\n",
    "\n",
    "    # Compute the indicator function (1 if bin edge >= y, else 0)\n",
    "    # We need to compare each y_value with bin_edges and broadcast correctly\n",
    "    indicators = (bin_edges[1:].unsqueeze(0) >= y_values.unsqueeze(1)).float()  # (N, 5000)\n",
    "\n",
    "    # Step 4: Compute bin widths\n",
    "    bin_widths = (bin_edges[1:] - bin_edges[:-1]).unsqueeze(0)  # (1, 5000)\n",
    "\n",
    "    # Step 5: Compute CRPS integral for each row\n",
    "    crps = torch.sum((cdf - indicators) ** 2 * bin_widths, dim=1)  # (N,)\n",
    "\n",
    "    return crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and transformed successfully. Shape of DataFrame: (78912, 22)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = ['ws_10m_loc_mean', 'ws_100m_loc_mean', 'power_t-96']\n",
    "target_column='power'\n",
    "\n",
    "entsoe = load_entsoe()\n",
    "entsoe = add_lagged_features(entsoe)\n",
    "entsoe = add_interval_index(entsoe)\n",
    "entsoe.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power</th>\n",
       "      <th>ws_10m_loc_1</th>\n",
       "      <th>ws_10m_loc_10</th>\n",
       "      <th>ws_10m_loc_2</th>\n",
       "      <th>ws_10m_loc_3</th>\n",
       "      <th>ws_10m_loc_4</th>\n",
       "      <th>ws_10m_loc_5</th>\n",
       "      <th>ws_10m_loc_6</th>\n",
       "      <th>ws_10m_loc_7</th>\n",
       "      <th>ws_10m_loc_8</th>\n",
       "      <th>...</th>\n",
       "      <th>ws_100m_loc_3</th>\n",
       "      <th>ws_100m_loc_4</th>\n",
       "      <th>ws_100m_loc_5</th>\n",
       "      <th>ws_100m_loc_6</th>\n",
       "      <th>ws_100m_loc_7</th>\n",
       "      <th>ws_100m_loc_8</th>\n",
       "      <th>ws_100m_loc_9</th>\n",
       "      <th>ws_100m_loc_mean</th>\n",
       "      <th>power_t-96</th>\n",
       "      <th>interval_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-02 00:00:00</th>\n",
       "      <td>595.0</td>\n",
       "      <td>2.7200</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.6400</td>\n",
       "      <td>3.1400</td>\n",
       "      <td>3.110</td>\n",
       "      <td>3.210</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.6500</td>\n",
       "      <td>...</td>\n",
       "      <td>5.39</td>\n",
       "      <td>5.200</td>\n",
       "      <td>5.8700</td>\n",
       "      <td>5.650</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.52</td>\n",
       "      <td>5.32</td>\n",
       "      <td>5.19</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 00:15:00</th>\n",
       "      <td>650.0</td>\n",
       "      <td>2.7975</td>\n",
       "      <td>1.74</td>\n",
       "      <td>2.6625</td>\n",
       "      <td>3.1325</td>\n",
       "      <td>3.185</td>\n",
       "      <td>3.235</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.7025</td>\n",
       "      <td>...</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.315</td>\n",
       "      <td>5.8825</td>\n",
       "      <td>5.645</td>\n",
       "      <td>4.81</td>\n",
       "      <td>4.64</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.16</td>\n",
       "      <td>1379.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 00:30:00</th>\n",
       "      <td>696.0</td>\n",
       "      <td>2.8750</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.6850</td>\n",
       "      <td>3.1250</td>\n",
       "      <td>3.260</td>\n",
       "      <td>3.260</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.7550</td>\n",
       "      <td>...</td>\n",
       "      <td>5.41</td>\n",
       "      <td>5.430</td>\n",
       "      <td>5.8950</td>\n",
       "      <td>5.640</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.76</td>\n",
       "      <td>5.34</td>\n",
       "      <td>5.13</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     power  ws_10m_loc_1  ws_10m_loc_10  ws_10m_loc_2  \\\n",
       "time                                                                    \n",
       "2016-01-02 00:00:00  595.0        2.7200           1.68        2.6400   \n",
       "2016-01-02 00:15:00  650.0        2.7975           1.74        2.6625   \n",
       "2016-01-02 00:30:00  696.0        2.8750           1.80        2.6850   \n",
       "\n",
       "                     ws_10m_loc_3  ws_10m_loc_4  ws_10m_loc_5  ws_10m_loc_6  \\\n",
       "time                                                                          \n",
       "2016-01-02 00:00:00        3.1400         3.110         3.210          3.11   \n",
       "2016-01-02 00:15:00        3.1325         3.185         3.235          3.11   \n",
       "2016-01-02 00:30:00        3.1250         3.260         3.260          3.11   \n",
       "\n",
       "                     ws_10m_loc_7  ws_10m_loc_8  ...  ws_100m_loc_3  \\\n",
       "time                                             ...                  \n",
       "2016-01-02 00:00:00          2.63        2.6500  ...           5.39   \n",
       "2016-01-02 00:15:00          2.65        2.7025  ...           5.40   \n",
       "2016-01-02 00:30:00          2.67        2.7550  ...           5.41   \n",
       "\n",
       "                     ws_100m_loc_4  ws_100m_loc_5  ws_100m_loc_6  \\\n",
       "time                                                               \n",
       "2016-01-02 00:00:00          5.200         5.8700          5.650   \n",
       "2016-01-02 00:15:00          5.315         5.8825          5.645   \n",
       "2016-01-02 00:30:00          5.430         5.8950          5.640   \n",
       "\n",
       "                     ws_100m_loc_7  ws_100m_loc_8  ws_100m_loc_9  \\\n",
       "time                                                               \n",
       "2016-01-02 00:00:00           4.87           4.52           5.32   \n",
       "2016-01-02 00:15:00           4.81           4.64           5.33   \n",
       "2016-01-02 00:30:00           4.75           4.76           5.34   \n",
       "\n",
       "                     ws_100m_loc_mean  power_t-96  interval_index  \n",
       "time                                                               \n",
       "2016-01-02 00:00:00              5.19      1428.0               1  \n",
       "2016-01-02 00:15:00              5.16      1379.0               2  \n",
       "2016-01-02 00:30:00              5.13      1399.0               3  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entsoe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training observations: 245376 | 77.76%\n",
      "# of validation observations: 35040 | 11.10%\n",
      "# of test observations: 35133 | 11.13%\n"
     ]
    }
   ],
   "source": [
    "train_end = \"2022-12-31 23:45:00\"\n",
    "validation_end = \"2023-12-31 23:45:00\"\n",
    "\n",
    "train, validation, test = to_train_validation_test_data(entsoe, train_end, validation_end)\n",
    "X_train, y_train = train[feature_columns], train[target_column]\n",
    "X_validation, y_validation = validation[feature_columns], validation[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "power               16676.00\n",
       "ws_10m_loc_1           13.85\n",
       "ws_10m_loc_10          14.03\n",
       "ws_10m_loc_2           14.85\n",
       "ws_10m_loc_3           14.72\n",
       "ws_10m_loc_4           15.00\n",
       "ws_10m_loc_5           14.06\n",
       "ws_10m_loc_6           14.13\n",
       "ws_10m_loc_7           12.39\n",
       "ws_10m_loc_8           12.97\n",
       "ws_10m_loc_9           12.04\n",
       "ws_10m_loc_mean        15.84\n",
       "ws_100m_loc_1          21.09\n",
       "ws_100m_loc_10         21.60\n",
       "ws_100m_loc_2          23.15\n",
       "ws_100m_loc_3          22.62\n",
       "ws_100m_loc_4          23.33\n",
       "ws_100m_loc_5          22.07\n",
       "ws_100m_loc_6          21.95\n",
       "ws_100m_loc_7          23.85\n",
       "ws_100m_loc_8          20.39\n",
       "ws_100m_loc_9          19.61\n",
       "ws_100m_loc_mean       24.66\n",
       "power_t-96          16676.00\n",
       "interval_index         96.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entsoe.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first_q...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS shape for first_q: torch.Size([8640])\n",
      "Updated max_nll_so_far to: 20.700092315673828 based on the current finite NLLs\n",
      "Replaced infinite values at indices [] in nll_torch_q with the value 20.700092315673828.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minu\\Documents\\master-thesis\\src\\analysis\\TabPFN_copy.py:359: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  crps_value, _ = quad(integrand, y_min, y_max)\n",
      "C:\\Users\\Minu\\Documents\\master-thesis\\src\\analysis\\TabPFN_copy.py:359: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  crps_value, _ = quad(integrand, y_min, y_max)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CRPS for first_q - CDF Linear: 878.3663\n",
      "Mean CRPS for first_q - Hybrid CDF: 3747.7258\n",
      "Mean CRPS for first_q - Normal: 836.3055\n",
      "Mean NLL for first_q - PDF Linear: 10.0371\n",
      "Mean NLL for first_q - PDF Hybrid: 11.9538\n",
      "Mean NLL for first_q - Normal: 16.3745\n",
      "✅ Processing complete. Results stored in `results_dict`.\n",
      "Processing second_q...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS shape for second_q: torch.Size([8736])\n",
      "Updated max_nll_so_far to: 18.114002227783203 based on the current finite NLLs\n",
      "Replaced infinite values at indices [] in nll_torch_q with the value 18.114002227783203.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minu\\Documents\\master-thesis\\src\\analysis\\TabPFN_copy.py:359: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  crps_value, _ = quad(integrand, y_min, y_max)\n",
      "C:\\Users\\Minu\\Documents\\master-thesis\\src\\analysis\\TabPFN_copy.py:359: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n",
      "  crps_value, _ = quad(integrand, y_min, y_max)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CRPS for second_q - CDF Linear: 708.6780\n",
      "Mean CRPS for second_q - Hybrid CDF: 1478.2360\n",
      "Mean CRPS for second_q - Normal: 637.9845\n",
      "Mean NLL for second_q - PDF Linear: 9.4458\n",
      "Mean NLL for second_q - PDF Hybrid: 8.7745\n",
      "Mean NLL for second_q - Normal: 10.8418\n",
      "✅ Processing complete. Results stored in `results_dict`.\n",
      "Processing third_q...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS shape for third_q: torch.Size([8832])\n",
      "Updated max_nll_so_far to: 20.73533058166504 based on the current finite NLLs\n",
      "Replaced infinite values at indices [] in nll_torch_q with the value 20.73533058166504.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minu\\Documents\\master-thesis\\src\\analysis\\TabPFN_copy.py:359: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  crps_value, _ = quad(integrand, y_min, y_max)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CRPS for third_q - CDF Linear: 743.8101\n",
      "Mean CRPS for third_q - Hybrid CDF: 1517.9408\n",
      "Mean CRPS for third_q - Normal: 691.4791\n",
      "Mean NLL for third_q - PDF Linear: 9.5101\n",
      "Mean NLL for third_q - PDF Hybrid: 8.5721\n",
      "Mean NLL for third_q - Normal: 11.7756\n",
      "✅ Processing complete. Results stored in `results_dict`.\n",
      "Processing fourth_q...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Minu\\Documents\\master-thesis\\.venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS shape for fourth_q: torch.Size([8832])\n",
      "Updated max_nll_so_far to: 21.09278106689453 based on the current finite NLLs\n",
      "Replaced infinite values at indices [] in nll_torch_q with the value 21.09278106689453.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Minu\\Documents\\master-thesis\\src\\analysis\\TabPFN_copy.py:359: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  crps_value, _ = quad(integrand, y_min, y_max)\n",
      "C:\\Users\\Minu\\Documents\\master-thesis\\src\\analysis\\TabPFN_copy.py:359: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  crps_value, _ = quad(integrand, y_min, y_max)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CRPS for fourth_q - CDF Linear: 1144.1682\n",
      "Mean CRPS for fourth_q - Hybrid CDF: 3596.0906\n",
      "Mean CRPS for fourth_q - Normal: 1134.9627\n",
      "Mean NLL for fourth_q - PDF Linear: 10.2689\n",
      "Mean NLL for fourth_q - PDF Hybrid: 9.9651\n",
      "Mean NLL for fourth_q - Normal: 22.1410\n",
      "✅ Processing complete. Results stored in `results_dict`.\n"
     ]
    }
   ],
   "source": [
    "# Train 4 models for each quarter\n",
    "# Define the train-validation splits\n",
    "splits = {\n",
    "    \"first_q\": {\n",
    "        \"train\": (\"2022-01-01\", \"2022-03-31\"),\n",
    "        \"validation\": (\"2023-01-01\", \"2023-03-31\"),\n",
    "        #\"train\": (\"2022-01-01\", \"2022-01-03\"),\n",
    "        #\"validation\": (\"2023-01-01\", \"2023-01-03\"),\n",
    "    },\n",
    "    \"second_q\": {\n",
    "        \"train\": (\"2022-04-01\", \"2022-06-30\"),\n",
    "        \"validation\": (\"2023-04-01\", \"2023-06-30\"),\n",
    "        #\"train\": (\"2022-01-04\", \"2022-01-06\"),\n",
    "        #\"validation\": (\"2023-01-04\", \"2023-01-06\"),\n",
    "    },\n",
    "    \"third_q\": {\n",
    "        \"train\": (\"2022-07-01\", \"2022-09-30\"),\n",
    "        \"validation\": (\"2023-07-01\", \"2023-09-30\"),\n",
    "    },\n",
    "    \"fourth_q\": {\n",
    "        \"train\": (\"2022-10-01\", \"2022-12-31\"),\n",
    "        \"validation\": (\"2023-10-01\", \"2023-12-31\"),\n",
    "        #\"validation\": (\"2023-12-26\", \"2023-12-31\")\n",
    "    }\n",
    "}\n",
    "# Initialize dictionary to store results\n",
    "results_dict = {}\n",
    "summary_stats_overall = {}\n",
    "\n",
    "# Loop through each quarter\n",
    "for quarter, dates in splits.items():\n",
    "    print(f\"Processing {quarter}...\")\n",
    "\n",
    "    # Extract train and validation data\n",
    "    X_train_q = X_train.loc[dates[\"train\"][0]:dates[\"train\"][1]]\n",
    "    y_train_q = y_train.loc[dates[\"train\"][0]:dates[\"train\"][1]]\n",
    "    \n",
    "    X_validation_q = X_validation.loc[dates[\"validation\"][0]:dates[\"validation\"][1]]\n",
    "    y_validation_q = y_validation.loc[dates[\"validation\"][0]:dates[\"validation\"][1]]\n",
    "\n",
    "    # Train model\n",
    "    model = TabPFNRegressor(device='auto', fit_mode='low_memory', random_state=42)\n",
    "    model.fit(X_train_q, y_train_q)\n",
    "\n",
    "    # Define quantiles\n",
    "    quantiles_custom = np.arange(0.1, 1, 0.1)\n",
    "    probabilities_q = quantiles_custom\n",
    "\n",
    "    # Predict\n",
    "    probs_val_q = model.predict(X_validation_q, output_type=\"full\", quantiles=quantiles_custom)\n",
    "    logits_q = probs_val_q[\"logits\"]\n",
    "    borders_q = probs_val_q[\"criterion\"].borders\n",
    "    all_quantiles_q = np.array(probs_val_q[\"quantiles\"])\n",
    "\n",
    "    # Convert y_validation to tensor\n",
    "    y_validation_q_torch = torch.tensor(y_validation_q.values, dtype=torch.float32)\n",
    "\n",
    "    # Compute CRPS and NLL using the 5000 logits and 5001 borders from TabPFN\n",
    "    crps_values_torch_q = compute_crps_pytorch(logits_q, borders_q, y_validation_q_torch)\n",
    "    print(f\"CRPS shape for {quarter}:\", crps_values_torch_q.shape)\n",
    "\n",
    "\n",
    "    # Initialize max_nll_so_far to a very small number\n",
    "    max_nll_so_far = float('-9999')\n",
    "    nll_torch_q = probs_val_q[\"criterion\"].forward(logits_q, y_validation_q_torch)\n",
    "\n",
    "    # Find the max ignoring inf and NaN\n",
    "    finite_nlls = nll_torch_q[torch.isfinite(nll_torch_q)]\n",
    "\n",
    "    if finite_nlls.numel() > 0:  # numel() = number of elements in finite_nlls\n",
    "        max_nll_so_far = max(max_nll_so_far, finite_nlls.max().item())\n",
    "        print(f\"Updated max_nll_so_far to: {max_nll_so_far} based on the current finite NLLs\")\n",
    "\n",
    "    # Find the indices of the infinite values\n",
    "    infinite_indices = torch.nonzero(~torch.isfinite(nll_torch_q)).squeeze()\n",
    "\n",
    "    # Replace infinite values with max_nll_so_far\n",
    "    infinite_values = nll_torch_q[infinite_indices]\n",
    "    nll_torch_q[~torch.isfinite(nll_torch_q)] = max_nll_so_far\n",
    "\n",
    "    # Print out the indices and the values that were replaced\n",
    "    print(f\"Replaced infinite values at indices {infinite_indices.tolist()} in nll_torch_q with the value {max_nll_so_far}.\")\n",
    "\n",
    "\n",
    "    # Fit tails\n",
    "    yt = entsoe[\"power\"]\n",
    "    quantile_10 = np.percentile(yt, probabilities_q * 100)\n",
    "    mu_left_asym, sigma_left_asym = fit_tail_distribution(quantile_10[:2], probabilities_q[:2])\n",
    "    mu_right_asym, sigma_right_asym = fit_tail_distribution(quantile_10[-2:], probabilities_q[-2:])\n",
    "\n",
    "    # Initialize lists for CRPS and NLL calculations of the 9 deciles\n",
    "    crps_cdf_linear_a_full_q = []\n",
    "    crps_hybrid_cdf_a_full_q = []\n",
    "    crps_normal_a_full_q = []\n",
    "    nll_pdf_linear_a_full_q = []\n",
    "    nll_pdf_hybrid_a_full_q = []\n",
    "    nll_normal_a_full_q = []\n",
    "    y_min, y_max = -10000., 20000.\n",
    "\n",
    "    if np.min(all_quantiles_q) < y_min:\n",
    "        y_min = np.min(all_quantiles_q)\n",
    "        print(y_min)\n",
    "    \n",
    "    if np.max(all_quantiles_q) > y_max:\n",
    "        y_max = np.max(all_quantiles_q)\n",
    "        print(y_max)\n",
    "\n",
    "\n",
    "    # Iterate through validation samples\n",
    "    for i in range(y_validation_q.shape[0]):\n",
    "        quantile_i = all_quantiles_q[:, i]\n",
    "        y_i = y_validation_q.iloc[i]\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        #cdf_linear, hybrid_cdf, crps_normal, pdf_linear, pdf_hybrid, nll_normal = evaluate(\n",
    "        #    quantile_i, probabilities_q, y_i, -20, 5, mu_left_asym, sigma_left_asym, mu_right_asym, sigma_right_asym\n",
    "        #)\n",
    "        cdf_linear, hybrid_cdf, crps_normal, pdf_linear, pdf_hybrid, nll_normal = evaluate(\n",
    "            quantile_i, probabilities_q, y_i, y_min, y_max, mu_left_asym, sigma_left_asym, mu_right_asym, sigma_right_asym\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        crps_cdf_linear_a_full_q.append(cdf_linear)\n",
    "        crps_hybrid_cdf_a_full_q.append(hybrid_cdf)\n",
    "        crps_normal_a_full_q.append(crps_normal)\n",
    "        nll_pdf_linear_a_full_q.append(pdf_linear)\n",
    "        nll_pdf_hybrid_a_full_q.append(pdf_hybrid)\n",
    "        nll_normal_a_full_q.append(nll_normal)\n",
    "\n",
    "    # Compute mean values\n",
    "    mean_crps_cdf_linear_q = np.mean(crps_cdf_linear_a_full_q)\n",
    "    mean_crps_hybrid_cdf_q = np.mean(crps_hybrid_cdf_a_full_q)\n",
    "    mean_crps_normal_q = np.mean(crps_normal_a_full_q)\n",
    "    mean_nll_pdf_linear_q = np.mean(nll_pdf_linear_a_full_q)\n",
    "    mean_nll_pdf_hybrid_q = np.mean(nll_pdf_hybrid_a_full_q)\n",
    "    mean_nll_normal_q = np.mean(nll_normal_a_full_q)\n",
    "\n",
    "    # Print mean values\n",
    "    print(f\"Mean CRPS for {quarter} - CDF Linear: {mean_crps_cdf_linear_q:.4f}\")\n",
    "    print(f\"Mean CRPS for {quarter} - Hybrid CDF: {mean_crps_hybrid_cdf_q:.4f}\")\n",
    "    print(f\"Mean CRPS for {quarter} - Normal: {mean_crps_normal_q:.4f}\")\n",
    "    print(f\"Mean NLL for {quarter} - PDF Linear: {mean_nll_pdf_linear_q:.4f}\")\n",
    "    print(f\"Mean NLL for {quarter} - PDF Hybrid: {mean_nll_pdf_hybrid_q:.4f}\")\n",
    "    print(f\"Mean NLL for {quarter} - Normal: {mean_nll_normal_q:.4f}\")\n",
    "\n",
    "    # Store results in dictionary\n",
    "    result_q = {\n",
    "        'CRPS Linear': crps_cdf_linear_a_full_q,\n",
    "        'CRPS Hybrid': crps_hybrid_cdf_a_full_q,\n",
    "        'CRPS Normal': crps_normal_a_full_q,\n",
    "        'CRPS (5000 quantiles)': crps_values_torch_q,\n",
    "        'NLL Linear': nll_pdf_linear_a_full_q,\n",
    "        'NLL Hybrid': nll_pdf_hybrid_a_full_q,\n",
    "        'NLL Normal': nll_normal_a_full_q,\n",
    "        'NLL (5000 quantiles)': nll_torch_q,\n",
    "        'y values': y_validation_q_torch,\n",
    "    }\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Convert to DataFrame and round values\n",
    "    results_q = pd.DataFrame(result_q).round(8)\n",
    "    results_dict[quarter] = results_q\n",
    "\n",
    "    summary_stats_q = {\n",
    "        'CRPS Linear': [mean_crps_cdf_linear_q, np.min(crps_cdf_linear_a_full_q), np.max(crps_cdf_linear_a_full_q), np.median(crps_cdf_linear_a_full_q)],\n",
    "        'CRPS Hybrid': [mean_crps_hybrid_cdf_q, np.min(crps_hybrid_cdf_a_full_q), np.max(crps_hybrid_cdf_a_full_q), np.median(crps_hybrid_cdf_a_full_q)],\n",
    "        'CRPS Normal': [mean_crps_normal_q, np.min(crps_normal_a_full_q), np.max(crps_normal_a_full_q), np.median(crps_normal_a_full_q)],\n",
    "        'CRPS (5000 quantiles)': [crps_values_torch_q.mean().item(), np.min(crps_values_torch_q.cpu().numpy()), np.max(crps_values_torch_q.cpu().numpy()), np.median(crps_values_torch_q.cpu().numpy())],\n",
    "        'NLL Linear': [mean_nll_pdf_linear_q, np.min(nll_pdf_linear_a_full_q), np.max(nll_pdf_linear_a_full_q), np.median(nll_pdf_linear_a_full_q)],\n",
    "        'NLL Hybrid': [mean_nll_pdf_hybrid_q, np.min(nll_pdf_hybrid_a_full_q), np.max(nll_pdf_hybrid_a_full_q), np.median(nll_pdf_hybrid_a_full_q)],\n",
    "        'NLL Normal': [mean_nll_normal_q, np.min(nll_normal_a_full_q), np.max(nll_normal_a_full_q), np.median(nll_normal_a_full_q)],\n",
    "        'NLL (5000 quantiles)': [nll_torch_q.mean().item(), np.min(nll_torch_q.cpu().numpy()), np.max(nll_torch_q.cpu().numpy()), np.median(nll_torch_q.cpu().numpy())],\n",
    "    }\n",
    "\n",
    "    # Convert the summary stats into a DataFrame\n",
    "    summary_stats_q_pd = pd.DataFrame(summary_stats_q, index=['Mean', 'Min', 'Max', 'Median'])\n",
    "    summary_stats_overall[quarter] = summary_stats_q_pd\n",
    "\n",
    "    # Print summary\n",
    "    print(\"✅ Processing complete. Results stored in `results_dict`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16676.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entsoe.power.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-3\n",
    "lag=96\n",
    "transformation = \"None\"\n",
    "\n",
    "# Table for metadata\n",
    "\n",
    "split_rows = []\n",
    "for quarter, dates in splits.items():\n",
    "    # Create a row with both 'train' and 'validation' in the same row\n",
    "    split_rows.append({\n",
    "        'quarter': quarter,\n",
    "        'train_start_date': dates['train'][0],\n",
    "        'train_end_date': dates['train'][1],\n",
    "        'validation_start_date': dates['validation'][0],\n",
    "        'validation_end_date': dates['validation'][1],\n",
    "        'random_seed': 42,\n",
    "        'num_splits': 4,\n",
    "        'epsilon': epsilon,\n",
    "        'lag': lag,\n",
    "        'transformation': transformation,\n",
    "        'features': feature_columns,\n",
    "        'model_type': \"TabPFNRegressor\",\n",
    "        'device': \"auto\",\n",
    "        'fit_mode': \"low_memory\",\n",
    "        \"ignore_pretaining_limits\": \"False\"\n",
    "    })\n",
    "\n",
    "meta_info_df = pd.DataFrame(split_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all DataFrames from results_dict into one big DataFrame\n",
    "all_quarters_df = pd.concat(results_dict.values(), ignore_index=True)\n",
    "\n",
    "# Compute mean, min, median, and max for all numeric columns\n",
    "summary_stats = all_quarters_df.describe().loc[['mean', 'min', '50%', 'max']].rename(index={'50%': 'median'})\n",
    "\n",
    "# Compute mean of \"CRPS (5000 quantiles)\" separately\n",
    "crps_mean = all_quarters_df[\"CRPS (5000 quantiles)\"].mean()\n",
    "\n",
    "# Compute mean of \"NLL (5000 quantiles)\" separately\n",
    "nll_mean = all_quarters_df[\"NLL (5000 quantiles)\"].mean()\n",
    "\n",
    "# Convert them into separate DataFrames\n",
    "crps_mean_df = pd.DataFrame({\"Metric\": [\"Mean CRPS (5000 quantiles)\"], \"Value\": [crps_mean]})\n",
    "nll_mean_df = pd.DataFrame({\"Metric\": [\"Mean NLL (5000 quantiles)\"], \"Value\": [nll_mean]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics and mean CRPS/NLL added to \"summary_stats\" sheet in ../../../OneDrive/Arbeit/HTWG/Master/results/TabPFN/results_ws10m_ws100m_pt_96_unstandardized_power.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define the output Excel file name\n",
    "output_excel_file = '../../../OneDrive/Arbeit/HTWG/Master/results/TabPFN/results_ws10m_ws100m_pt_96_unstandardized_power.xlsx'\n",
    "#output_excel_file = '../../../OneDrive/Arbeit/HTWG/Master/results/TabPFN/tests/2_results_ws10m_ws100m_pt_96.xlsx'\n",
    "\n",
    "# Write everything to the Excel file\n",
    "with pd.ExcelWriter(output_excel_file) as writer:\n",
    "    # Write each split DataFrame to its own sheet\n",
    "    for quarter, df in results_dict.items():\n",
    "        df.to_excel(writer, sheet_name=quarter, index=False)\n",
    "        summary_stats_overall[quarter].to_excel(writer, sheet_name=quarter, startcol=len(df.columns) + 3, index=True)\n",
    "\n",
    "    # Write the meta_info DataFrame\n",
    "    meta_info_df.to_excel(writer, sheet_name='meta_info', index=False)\n",
    "    \n",
    "    # Write summary statistics\n",
    "    summary_stats.to_excel(writer, sheet_name=\"summary_stats\")\n",
    "    \n",
    "    # Append the mean CRPS and NLL separately below the summary stats\n",
    "    crps_mean_df.to_excel(writer, sheet_name=\"summary_stats\", startrow=len(summary_stats) + 2, index=False)\n",
    "    nll_mean_df.to_excel(writer, sheet_name=\"summary_stats\", startrow=len(summary_stats) + 4, index=False)\n",
    "\n",
    "print(f'Summary statistics and mean CRPS/NLL added to \"summary_stats\" sheet in {output_excel_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done writing the results of ws_10m, ws_100m, P_t-96_unstandardized_power to excel file\n"
     ]
    }
   ],
   "source": [
    "print(\"done writing the results of ws_10m, ws_100m, P_t-96_unstandardized_power to excel file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z=(y-mu)/sigma or y= mu + sigma * z, mu = 3760, sigma = 3388, TabPFN lowest and highest bin borders are z_min = - 98, z_max = 86. Hence\n",
    "y_min = 3760 - 3388*98 = -328'264, y_max = 3760 + 3388*86 = 295'128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3388.4129789136728"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entsoe[\"power\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-328264"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_min = 3760 - 3388*98\n",
    "y_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295128"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_max = 3760 + 3388*86\n",
    "y_max"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
